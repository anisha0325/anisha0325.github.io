<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="VITAL: More Understandable Feature Visualization through Distribution Alignment and Relevant Information Flow"/>
  <meta property="og:description" content="A new method to generate interpretable feature visualizations for neural networks by aligning feature distributions and tracing relevant information flow."/>  
  <meta property="og:url" content="https://adagorgun.github.io/VITAL-Project/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/teaser.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>More Understandable Feature Visualization through Distribution Alignment and Relevant Information Flow</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <style>
    /* Prevent carousel from showing next/previous slides */
    #results-carousel {
      overflow: hidden;
    }
  
    #results-carousel .item {
      flex: 0 0 100% !important;
      max-width: 100% !important;
    }
  
    /* Hide navigation arrows */
    .carousel-nav-left,
    .carousel-nav-right {
      display: none !important;
    }
  </style>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <img src="static/images/final_favicon.png" alt="V" style="height: 1.1em; vertical-align: -0.15em; position: relative; left: 0.3em; margin-right: 0.05em;">
              <span style="color: #006c66;">ITAL</span>: More Understandable Feature Visualization through Distribution Alignment and Relevant Information Flow
            </h1>
            <h1 class="title is-3">
                ICCV 2025          </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://adagorgun.github.io/" target="_blank">Ada Görgün</a>,</span>
                <span class="author-block">
                  <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele/" target="_blank">Bernt Schiele</a>,</span>
                  <span class="author-block">
                    <a href="http://explainablemachines.com/members/jonas-fischer.html" target="_blank">Jonas Fischer</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"> Max Planck Institute for Informatics, Saarland Informatics Campus, Germany <br></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2503.22399.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>


                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/adagorgun/VITAL" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2503.22399" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser Figure-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/teaser.png" 
      alt="Learning from model weights" 
      class="blend-img-background center-image" 
      style="max-width: 100%; height: auto;">
      <h2 class="subtitle has-text-centered">
       Unlike traditional feature visualization (FV) methods, which often produce
       artifacts or repetitive patterns, VITAL generates more understandable visualizations through feature distribution matching. Our approach scales effectively to modern architectures
       (rows), generalizes well across diverse classes (columns), and better captures meaningful network representations.      </h2>
    </div>
  </div>
</section>
<!-- End Teaser Figure -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Neural networks are widely adopted to solve complex and challenging tasks. Especially in high-stakes decision-making, understanding their reasoning process is crucial, yet proves challenging for modern deep networks. Feature visualization (FV) is a powerful tool to decode what information neurons are responding to and hence to better understand the reasoning behind such networks. In particular, in FV we generate human-understandable images that reflect the information detected by neurons of interest. However, current methods often yield unrecognizable visualizations, exhibiting repetitive patterns and visual artifacts that are hard to understand for a human. To address these problems, we propose to guide FV through statistics of real image features combined with measures of relevant network flow to generate prototypical images. Our approach yields human-understandable visualizations that both qualitatively and quantitatively improve over state-of-the-art FVs across various architectures. As such, it can be used to decode which information the network uses, complementing mechanistic circuits that identify where it is encoded.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Method Class Neuron Video -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content" style="text-align: justify;">
          <h2 class="title is-3 has-text-centered">Class Neuron Visualization</h2>
          <p>
            Class neuron visualization aims to reveal what a neural network "sees" when it thinks about a specific class (e.g., "dog" or "airplane"). This is done by generating an image that maximally activates the output neuron corresponding to that class. The resulting visualization gives insight into the features the model associates with that category—such as shapes, textures, or patterns. VITAL enhances this by aligning these visualizations with real-world feature distributions, resulting in clearer and more realistic class representations. This is achieved by matching the generated image's feature distribution to that of real images from the same class through the sort matching algorithm. The result is a more interpretable and meaningful visualization that can help us understand how the model perceives different classes.
          </p>
          <p>
            <video poster="" id="tree" autoplay="" controls="" muted="" loop="" height="100%">
              <!-- Your video here -->
              <source src="static/videos/class_neurons.mp4" type="video/mp4">
            </video>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      
      <!-- Carousel Title -->
      <h2 class="title is-3 has-text-centered">Qualitative Results Across Architectures</h2>

      <!-- Carousel -->
      <div id="results-carousel" class="carousel">
        <div class="item has-text-centered" style="margin-bottom: 2rem;">
          <img src="static/images/class1.png" alt="MY ALT TEXT" class="is-inline-block" style="max-width: 90%; height: auto;" />
          <h2 class="subtitle">ResNet50</h2>
        </div>
        <div class="item has-text-centered" style="margin-bottom: 2rem;">
          <img src="static/images/class2.png" alt="MY ALT TEXT" class="is-inline-block" style="max-width: 90%; height: auto;" />
          <h2 class="subtitle">ConvNeXt-base</h2>
        </div>
        <div class="item has-text-centered" style="margin-bottom: 2rem;">
          <img src="static/images/class3.png" alt="MY ALT TEXT" class="is-inline-block" style="max-width: 90%; height: auto;" />
          <h2 class="subtitle">DenseNet121</h2>
        </div>
        <div class="item has-text-centered" style="margin-bottom: 2rem;">
          <img src="static/images/class4.png" alt="MY ALT TEXT" class="is-inline-block" style="max-width: 90%; height: auto;" />
          <h2 class="subtitle">ViT-L-16</h2>
        </div>
        <div class="item has-text-centered" style="margin-bottom: 2rem;">
          <img src="static/images/class5.png" alt="MY ALT TEXT" class="is-inline-block" style="max-width: 90%; height: auto;" />
          <h2 class="subtitle">ViT-L-32</h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Image carousel -->

<!-- End Method Class Neuron Video -->

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Quantitative Results</h2>
    <figure class="image" style="max-width: 500px; margin: 0 auto;">
      <img src="static/images/table.png" alt="Quantitative Results Table">
      <figcaption class="has-text-centered is-size-7 mt-2">
        <em>Comparison of methods on different architectures trained on ImageNet. FID scores, CLIP Zero-shot prediction scores, and top-1 accuracy are reported. <b>Bold</b> and <u>underlined</u> indicate best and second-best results, respectively.</em>
      </figcaption>
    </figure>
  </div>
</section>






<!-- Method Intermediate Neuron Video -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content" style="text-align: justify;">
          <h2 class="title is-3 has-text-centered">Intermediate Neuron Visualization</h2>
          <p>
            Intermediate neuron visualization focuses on understanding how information is represented deep inside the network, rather than just at the classification layer. These internal neurons often respond to abstract concepts like "fur texture" or "wheel shapes," even if they're not directly tied to a class. By visualizing what activates these hidden neurons, we can uncover emergent concepts and compositional features the model builds up to make decisions. VITAL improves this process by filtering neurons based on their relevance and by guiding the visualizations with real feature statistics—leading to more meaningful and interpretable representations. Instead of just maximizing neuron activation like in traditional methods, VITAL traces how much relevant information flows from the neuron toward the model’s final decision for that class and aligns the feature distribution of generated images with the feature distribution of real images that activates the target neuron the most.
          </p>
          <p>
            <video poster="" id="tree" autoplay="" controls="" muted="" loop="" height="100%">
              <!-- Your video here -->
              <source src="static/videos/inner_neurons.mp4" type="video/mp4">
            </video>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Method Intermediate Neuron Video -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      
      <!-- Carousel Title -->
      <h2 class="title is-3 has-text-centered">Qualitative Results for ResNet50</h2>

      <!-- Carousel -->
      <div id="results-carousel" class="carousel">
        <div class="item has-text-centered" style="margin-bottom: 2rem;">
          <img src="static/images/inner1.png" alt="MY ALT TEXT" class="is-inline-block" style="max-width: 90%; height: auto;" />
        </div>
        <div class="item has-text-centered" style="margin-bottom: 2rem;">
          <img src="static/images/inner2.png" alt="MY ALT TEXT" class="is-inline-block" style="max-width: 90%; height: auto;" />
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Image carousel -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{gorgun2025vitalunderstandablefeaturevisualization,
      title={VITAL: More Understandable Feature Visualization through Distribution Alignment and Relevant Information Flow}, 
      author={Ada Gorgun and Bernt Schiele and Jonas Fischer},
      year={2025},
      eprint={2503.22399},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.22399}, 
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
